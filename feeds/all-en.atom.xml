<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Phil Elson - Software | Science | Python</title><link href="https://pelson.github.io/" rel="alternate"></link><link href="https://pelson.github.io/feeds/all-en.atom.xml" rel="self"></link><id>https://pelson.github.io/</id><updated>2017-03-16T00:00:00+00:00</updated><entry><title>Playing with Randall Munroe's XKCD handwriting</title><link href="https://pelson.github.io/2017/xkcd_font/" rel="alternate"></link><published>2017-03-16T00:00:00+00:00</published><updated>2017-03-16T00:00:00+00:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2017-03-16:/2017/xkcd_font/</id><summary type="html">&lt;p&gt;The XKCD font (as used by matplotlib et al.) recently &lt;a href="https://github.com/ipython/xkcd-font/pull/13"&gt;got an update&lt;/a&gt; to include lower-case characters.
For some time now I have been aware of a handwriting sample produced by Randall Munroe (XKCD's creator) that I was interested in exploring.
The ultimate aim is to automatically produce a font-file …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The XKCD font (as used by matplotlib et al.) recently &lt;a href="https://github.com/ipython/xkcd-font/pull/13"&gt;got an update&lt;/a&gt; to include lower-case characters.
For some time now I have been aware of a handwriting sample produced by Randall Munroe (XKCD's creator) that I was interested in exploring.
The ultimate aim is to automatically produce a font-file using open source tools, and to learn a few things along the way.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;The thing about fonts is that there is actually a lot going on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;spacing&lt;/strong&gt; - the whitespace around a character&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;kering&lt;/strong&gt; - special case whitespace adjustments (e.g. notice the space between "Aw" is much closer than between "As")&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hinting&lt;/strong&gt; - techniques for improved rasterization at low resolution&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;liatures&lt;/strong&gt; - special pairs/groups of characters. The traditional example is ffi, but for hand-writing, any character combination is plausible.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The raw material I'm going to use is a scan produced by XKCD author Randall Munroe containing many charaters, as well as some spacing and ligature information.
Importantly, all of the glyphs have been written at the same scale and using the same pen.
These two details are important, as they will allow us to derive appropriate spacing and kerning information, and should result in a font that is well balanced.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="./../../images/xkcd-font/full-small.png"&gt;&lt;/p&gt;
&lt;p&gt;(full image available on GitHub at https://github.com/ipython/xkcd-font/issues/9#issuecomment-127412261)&lt;/p&gt;
&lt;p&gt;Notice some interesting features of this sample, including ligatures (notice that "LB" is a single mark) and
kerning (see the spacing of "VA"):&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="./../../images/xkcd-font/small-detail.png"&gt;&lt;/p&gt;
&lt;p&gt;There are a few useful articles already out there on this topic, particularly one from 2010 regarding the creation of fonts from a &lt;a href="http://scruss.com/blog/2010/05/09/creating-a-truetype-font-from-your-handwriting-with-your-scanner-your-printer-and-fontforge/"&gt;hand-written sample&lt;/a&gt;.
Notably though, there isn't much that is automated - this is a rub for me, as without automation it is challenging for others to contribute to the font in an open and diffable way.&lt;/p&gt;
&lt;p&gt;Let's get stuck in by separating each of the glyphs from the image into their own image.&lt;/p&gt;
&lt;p&gt;{% notebook ../field_notes/xkcd/part1.ipynb %}&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this initial phase, we haven't done anything particularly clever - we've simply loaded in the image,
taken a subset, and used scipy's image labelling capabilities to understand what the labelling process looks like.
Originally I had planned to try to separate some of the glyphs that were obviously fused together - I'd even gone as far
as protyping using a filter to disolve the outline (so that I could separate the labels) and then subsequently growing the
labels again back to their original form (but keeping the separate labels). This technique worked, but it produced shapes that
weren't perfect, and the complexities (e.g. handling of bits that dissapeared, such as dots on the letter "i") weren't worth it.&lt;/p&gt;
&lt;p&gt;In the next phase, we will use the techinque shown here to generate individual image files. In addition, we will apply some heuristics
merge back together glyphs such as the dot and comma of a semi-colon.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The next article in this series is entitled&lt;/em&gt;: &lt;strong&gt;&lt;a href="./xkcd_font_pt2.md"&gt;Merging together labels, and generating label images&lt;/a&gt;&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;Follow up items (some not yet written):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Blending together obvious glyphs&lt;/li&gt;
&lt;li&gt;Converting to vector&lt;/li&gt;
&lt;li&gt;Generating the font with font-forge&lt;/li&gt;
&lt;/ul&gt;</content><category term="XKCD"></category><category term="fonts"></category><category term="Python"></category></entry><entry><title>Mounting a FUSE filesystem in Heroku</title><link href="https://pelson.github.io/2017/heroku_fuse_mount/" rel="alternate"></link><published>2017-02-06T00:00:00+00:00</published><updated>2017-02-06T00:00:00+00:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2017-02-06:/2017/heroku_fuse_mount/</id><summary type="html">&lt;p&gt;This evening I'm going to take a different approach to how I would normaly blog.&lt;/p&gt;
&lt;p&gt;Rather than reporting the results of a technical investigation or highlighting a new/shiny package, I wanted to
paint a realistic picture of the technical exploration process.&lt;/p&gt;
&lt;p&gt;As it happens, this particular investigation consumed a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This evening I'm going to take a different approach to how I would normaly blog.&lt;/p&gt;
&lt;p&gt;Rather than reporting the results of a technical investigation or highlighting a new/shiny package, I wanted to
paint a realistic picture of the technical exploration process.&lt;/p&gt;
&lt;p&gt;As it happens, this particular investigation consumed a couple of hours and appears to have drawn an unsucessful
result. Despite this, the learnings are invaluable as they will be directly and immediately applicable to other areas of my work.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;h2&gt;The problem&lt;/h2&gt;
&lt;p&gt;I run a number of services on Heroku. It's an amazing platform for rapid prototyping of (mostly web) applications. The beauty of Heroku is the ease of deployment as well as its availablity, scalability and cost.&lt;/p&gt;
&lt;p&gt;I have one particular application on Heroku that draws statistics from a moderately sized, slow paced, filestore.
Unfortunately, since all storage on Heroku is ephermeral, that means I must fetch the data at least once every 24 hours (when Heroku restarts the container) to recompute my statistics (which are cached for application performance). Fetching the data is a costly operation, and I'd rather avoid it if possible.&lt;/p&gt;
&lt;p&gt;Instead of fetching the data on a daily basis, I'd like to have a third-party filestore that can be used from within the Heroku application for its statistics generation.
I'd also like to use the store for the cache (which itself can be mem-cached/cached to the ephemeral Heroku disk).
In order to invalidate the cache I will want an efficient means of getting a checksum or pertinent fstat info.
The detail here isn't important, suffice to say I'd just like &lt;em&gt;some&lt;/em&gt; persistence my source data for an otherwise stateless application.&lt;/p&gt;
&lt;h2&gt;The proposal&lt;/h2&gt;
&lt;p&gt;Having read a little in the past about libfuse, it seemed that using FUSE to mount a networked resource would be a great match for the problem.
Because I have a few GB lying around on Dropbox, and there appeared to be a python based Dropbox-fuse client (ff4d), I decided to investigate the feasibility of mounting my Dropbox directory inside my Heroku application. 
Other options would have worked just as a well, including an S3 bucket (with s3fs-fuse), or a direct ssh connection (with sshfs).&lt;/p&gt;
&lt;p&gt;I had also read a little bit about Heroku's new Docker container registry and runtime environment (https://devcenter.heroku.com/articles/container-registry-and-runtime) and thought this would be a great opportunity to shorten the deployment cycle.&lt;/p&gt;
&lt;h2&gt;The findings&lt;/h2&gt;
&lt;h3&gt;Mounting a dropbox directory through ff4d&lt;/h3&gt;
&lt;p&gt;The first step is to get FUSE up-and-running on my own machine (OSX). libfuse is the first thing in a while
that I've not been able to get hold of through conda, so I ended up dusting off my homebrew installation and going through
a pretty heafty update. Once complete I tried:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew install libfuse
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With no success. Turns out that osxfuse is a thing, and so I:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew install osxfuse
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Again, no luck, but homebrew does point me in the direction of a cask version (pre-compiled, rather than building it from source on my machine). I'm pretty keen to get going with this, so I go ahead and install the casked version:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew install Caskroom/cask/osxfuse
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After a short wait, the result is positive, and it looks like I have a local FUSE installation.&lt;/p&gt;
&lt;p&gt;Next, I want to try it out. Rather than choose to do somethign simple, I go straight for the jugular and try to get a dropbox FUSE mount working. I clone https://github.com/realriot/ff4d and get myself set up with a legacy python installation (in a clean environment named "ff4d_py2":&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;conda create -n ff4d_py2 python=2 pip
source activate ff4d_py2
pip install dropbox
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I take a note of the dependency on a legacy version of Python, and vow to submit a merge request making the codebase python (3) compatible should I want to turn this proof-of-concept into something more.&lt;/p&gt;
&lt;p&gt;Next, I create a "Dropbox API" -&amp;gt; "Full Dropbox" application on https://www.dropbox.com/developers/apps/create and run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;python getDropboxAccessToken.py
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I follow on the on-screen prompts and am eventually rewarded with an OAuth token that I store securely.
With my token in hand:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mkdir foobar
python ff4d.py  ./foobar -ap &amp;lt;my_token&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Gives me a directory called foobar containing my dropbox content, and reminds me that I can delete nearly a GB of images that
were shared with colleagues on my dropbox account. As I delete the files (&lt;code&gt;rm -rf ./foobar/my_image_directory&lt;/code&gt;) I'm aware that
there are a number of 404 error type messages being logged by ff4d.py - I take a note that there is something that needs deeper investigation here.&lt;/p&gt;
&lt;p&gt;So there we have it, a locally mounted dropbox folder sitting on my OSX machine, thanks to ff4d.
Now, I want to create a webapp that can show the contents of my directory (as a proof-of-concept), and to replicate this
setup in a docker container, and then ultimately in a Heroku web app deployment.&lt;/p&gt;
&lt;h3&gt;Creating the webapp&lt;/h3&gt;
&lt;p&gt;I'm a big fan of &lt;a href="http://www.tornadoweb.org/en/stable/"&gt;tornado&lt;/a&gt;, and since the application that could benefit from
access to my dropbox mount is also using tornado I put together a quick web-app to browse the directory.&lt;/p&gt;
&lt;p&gt;I'd assumed that creating a http handler that allows directory listing would be built-in to tornado, but it appears not,
so I ended up re-using code from https://github.com/imom0/SimpleTornadoServer/blob/master/SimpleTornadoServer.py (BSD-3)
to allow me to navigate my directories from within the webapp.&lt;/p&gt;
&lt;p&gt;In order for my webapp and ff4d mount to be run on the same process within Heroku (not the only option - it is easy enough to create new processes on Heroku, I just like the ability to control them all from a single process) I need to get the webapp to
mount the dropbox directory itself.
Since mounting through ff4d is blocking, I am going to need to run one 2 IOLoops, one on the main thread (for tornado) and the other in a ThreadPoolExecutor managed thread (for ff4d).&lt;/p&gt;
&lt;p&gt;Getting another thread to run a blocking script whilst still running a responsive tornado main IOLoop thread is something I have done a few times now.
In other situations I have wanted to commuicate through Kafka ((example)[http://stackoverflow.com/a/40602866/741316]) in my tornado application, and in another application I wanted the ability to (optionally) spawn a Dask scheduler, workers and client. Truth be told, in most of these situations processes are a better choice, but I digress.&lt;/p&gt;
&lt;p&gt;Getting a tornado webapp to run a blocking process in another thread is surprisingly easy.
We need a ThreadPool, and an asyncronous function that can run on the main thread:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;@tornado.gen.coroutine
def async(executor, function, *args, **kwargs):
    yield executor.submit(function, *args, **kwargs)

thread_pool = ThreadPoolExecutor(1)
tornado.ioloop.IOLoop.current().spawn_callback(async, thread_pool, start_mount, mount_dir=mount_dir)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The function itself is fairly trivial, and simply spawns a sub-process which mounts my dropbox directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def start_mount(mount_dir):
    dropbox_token = os.environ.get(&amp;#39;DROPBOX_TOKEN&amp;#39;)
    if not os.path.exists(mount_dir):
        os.mkdir(mount_dir)
    subprocess.check_call([sys.executable, &amp;#39;ff4d.py&amp;#39;, mount_dir, &amp;#39;-ap&amp;#39;, dropbox_token])
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With all of this in place, I'm able to fire up my tornado webapp, and navigate my dropbox content from within the browser.&lt;/p&gt;
&lt;p&gt;The complete code can be found at https://github.com/pelson/heroku-with-dropbox-mount.&lt;/p&gt;
&lt;h3&gt;Build &amp;amp; deploy with Docker&lt;/h3&gt;
&lt;p&gt;Groundwork complete, we now want to put some scaffolding around our proof-of-concept so that we can easily test and deploy
the application on Heroku.&lt;/p&gt;
&lt;p&gt;As I mentioned at the beginning, I recently read-up about the new docker based Heroku deployment option, and want to give it a shot.
Since I'm on OSX, I fire up docker-machine (needed an update) and get started with writing my Dockerfile.&lt;/p&gt;
&lt;p&gt;I was aware of Continuum's Docker images, and so reached for that as the base image, before extending to my requirements.
There is nothing earth-shattering about the Dockerfile I produced (available at https://github.com/pelson/heroku-with-dropbox-mount),
and my build -&amp;gt; test workflow looks like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker build --tag=docker_webapp_test
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker run -p 5000:5000 -e PORT=5000 -e DROPBOX_TOKEN=&amp;lt;my_token&amp;gt; -t -i docker_webapp_test
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As mentioned, I'm using docker-machine to run docker - it essentially manages a VirtualBox machine to run a suitable
host OS for docker. This means that even though the the port was forwarded in my &lt;code&gt;docker run&lt;/code&gt; call, it won't be visible to me on localhost. To find out what IP my machine is running on, we can call &lt;code&gt;docker-machine ip &amp;lt;machine_name&amp;gt;&lt;/code&gt;.
It is this address (+":5000") that I use to see the webapp in my browser.&lt;/p&gt;
&lt;p&gt;Things are starting to come together nicely, except the mount of my device was failing with messages similar to:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Starting FUSE...
fuse: failed to open /dev/fuse: Operation not permitted
[ERROR] Failed to start FUSE... (Traceback (most recent call last):
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It turns out that we need elevated priveledges to mount a FUSE device within docker.
Adding &lt;code&gt;--cap-add SYS_ADMIN&lt;/code&gt; and &lt;code&gt;--device /dev/fuse&lt;/code&gt; to docker should be enough (though it appears there was once a bug in docker that meant the container needed to be run with &lt;em&gt;full&lt;/em&gt; priveledges).
I make a note of this as a potential problem for our Heroku deployment.&lt;/p&gt;
&lt;p&gt;Finally, I'm able to launch my docker image and navigate my dropbox content through my web app.
The final step is to push this image to Heroku:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;heroku container:push web
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After a considerable amount of time waiting for all of the image layers to upload I get an error in my heroku logs.
Simply changing CMD to something that should obviously work (&lt;code&gt;ls -ltr&lt;/code&gt;) I get a similar error:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;2017-02-07T11:13:48.755363+00:00 heroku[web.1]: State changed from crashed to starting
2017-02-07T11:13:56.607038+00:00 heroku[web.1]: Starting process with command `/bin/sh -c \&amp;quot;ls\ -ltr\&amp;quot;`
2017-02-07T11:13:59.043845+00:00 app[web.1]: Error: No such file or directory
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It may be a red-herring, but the command escaping looks a little off.
I iterate further with just &lt;code&gt;ls&lt;/code&gt; as the CMD, but get the same error.
Iterating takes about 30s, so a trial and error approach to solving the problem is proving tedious - I would love to be able to reproduce the issue locally to shorten the loop.
I try removing the quotes within the Dockerfile's CMD section and I ensure that PATH is correctly set - still nothing.&lt;/p&gt;
&lt;p&gt;I look back at the docs learn about docker-compose - looks like an interesting tool for managing processes in a similar way to the Proc file within Heroku - definitely something to note for future exploration.&lt;/p&gt;
&lt;p&gt;With frustration setting in, I roll back to the Dockerfile provided in the documentation.
This uses alpine as is base and takes some time to upload, but eventually I'm able to confirm that I can at least run CMD on Heroku with that Dockerfile.
Whilst I'd love to understand what is wrong different between Continuum's image and the alpine image (other than the whole OS), my focus on getting my proof-of-concept up and running on Heroku, so I change tactic and update my Dockerfile derive FROM the alpine base image.&lt;/p&gt;
&lt;p&gt;After a little more iteration, I eventually manage to use alpine as the base for my webapp's image (including installing ca-certificates to prevent urllib from raising a CERTIFICATE_VERIFY_FAILED exception). I push the image to heroku, and define the DROPBOX_TOKEN environment variable that my code expects in the heroku web portal, but alas there is a problem with the FUSE mount:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;2017-02-07T14:36:06.997268+00:00 app[web.1]: Starting FUSE...
2017-02-07T14:36:06.997269+00:00 app[web.1]: [ERROR] Failed to start FUSE... (Traceback (most recent call last):
2017-02-07T14:36:06.997270+00:00 app[web.1]:   File &amp;quot;ff4d/ff4d.py&amp;quot;, line 812, in &amp;lt;module&amp;gt;
2017-02-07T14:36:06.997271+00:00 app[web.1]:     FUSE(Dropbox(ar), mountpoint, foreground=args.background, debug=debug_fuse, sync_read=True, allow_other=allow_other, allow_root=allow_root)
2017-02-07T14:36:06.997271+00:00 app[web.1]:   File &amp;quot;/opt/webapp/ff4d/fuse.py&amp;quot;, line 405, in __init__
2017-02-07T14:36:06.997272+00:00 app[web.1]:     raise RuntimeError(err)
2017-02-07T14:36:06.997273+00:00 app[web.1]: RuntimeError: 1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In addition, there is a message in the log along the lines of:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;fuse&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;found&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="k"&gt;try&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;modprobe fuse&amp;#39;&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It may be as I feared: Heroku doesn't currently support FUSE mounts.&lt;/p&gt;
&lt;p&gt;In order to get one final datapoint, I put together the equivalent Dockerfile for ubuntu rather than alpine. Unfortunately the results are the same.&lt;/p&gt;</content><category term="Heroku"></category><category term="FUSE"></category><category term="python"></category><category term="docker"></category></entry><entry><title>Building a matrix of conda distributions with conda-build-all</title><link href="https://pelson.github.io/2015/conda_build_all/" rel="alternate"></link><published>2015-12-09T12:00:00+00:00</published><updated>2015-12-09T12:00:00+00:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2015-12-09:/2015/conda_build_all/</id><summary type="html">&lt;p&gt;Introducing &lt;code&gt;conda-build-all&lt;/code&gt;, a tool which extends &lt;code&gt;conda-build&lt;/code&gt; to provide powerful build
matrix capabilities.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;Repositories such as &lt;a href="https://github.com/ioos/conda-recipes"&gt;conda-forge/stages-recipes&lt;/a&gt;, &lt;a href="https://github.com/SciTools/conda-recipes-scitools"&gt;SciTools/conda-recipes-scitools&lt;/a&gt; and &lt;a href="https://github.com/ioos/conda-recipes"&gt;ioos/conda-recipes&lt;/a&gt; exist to provide a set of conda recipes, and ultimately, channels from which users can access the product of &lt;code&gt;conda-build&lt;/code&gt;-ing those recipes.
The build phase of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Introducing &lt;code&gt;conda-build-all&lt;/code&gt;, a tool which extends &lt;code&gt;conda-build&lt;/code&gt; to provide powerful build
matrix capabilities.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;Repositories such as &lt;a href="https://github.com/ioos/conda-recipes"&gt;conda-forge/stages-recipes&lt;/a&gt;, &lt;a href="https://github.com/SciTools/conda-recipes-scitools"&gt;SciTools/conda-recipes-scitools&lt;/a&gt; and &lt;a href="https://github.com/ioos/conda-recipes"&gt;ioos/conda-recipes&lt;/a&gt; exist to provide a set of conda recipes, and ultimately, channels from which users can access the product of &lt;code&gt;conda-build&lt;/code&gt;-ing those recipes.
The build phase of all of these repositories looks very similar: a tool (ObviousCI) computes the build matrix, builds those distributions which haven't already been built, and then uploads them to their respective channels.
The functionality is tried and tested, and has been powering these repositories for over a year with huge success, however, I recently had need to use this functionality without wanting to upload the built distributions to &lt;a href="http://conda.anaconda.org"&gt;conda.anaconda.org&lt;/a&gt; and found the tool didn't &lt;em&gt;quite&lt;/em&gt; fit the bill.
Additionally, having originally cobbled together ObviousCI with string and sticky-tape to prove the concept of a continuously integrated repo of recipes, I didn't have huge confidence in its ability to function between python/conda/conda-build upgrades.&lt;/p&gt;
&lt;p&gt;As a result, I have re-factored the build part of ObviousCI into a general purpose library which can now be used for the original "conda recipe repository" usecase as much as it can for the ad-hoc "just build this" usecase. Critically, the most significant part of this re-factoring was adding a huge array of unit and integration tests which can be used to ensure expected behaviour is unchanged through future dependency version upgrades.&lt;/p&gt;
&lt;p&gt;The new CLI is &lt;code&gt;conda-build-all&lt;/code&gt; (BSD license) and is developed at &lt;a href="https://github.com/SciTools/conda-build-all"&gt;SciTools/conda-build-all&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;The build matrix&lt;/h3&gt;
&lt;p&gt;So what does a build matrix actually look like? Let's jump in at the deep-end and look at a package which has a numpy C-API dependency.
Whilst numpy's ABI is (intended to be) &lt;a href="http://stackoverflow.com/a/18369312/741316"&gt;forward-compatible&lt;/a&gt;, in practice it is safer to compile against a specific version and "pin" the distribution to that version.
Essentially, that means we need to build our recipe N times, where N is the number of numpy versions we wish to support.
Of course, the same is true for Python itself, leading to a permutation problem of up to &lt;code&gt;NxM&lt;/code&gt; builds (N: number of supported numpy versions; M: number of supported Python versions).&lt;/p&gt;
&lt;p&gt;The current conda recipe form for such a package looks like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;package&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;my_library&lt;/span&gt;
    &lt;span class="n"&gt;version&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="n"&gt;requirements&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;python&lt;/span&gt;
        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;x&lt;/span&gt;
    &lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;python&lt;/span&gt;
        &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;x&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Whilst I believe there is &lt;a href="https://github.com/conda/conda-build/pull/650"&gt;room for improvement&lt;/a&gt; in the recipe definition, it is still pretty easy to define a complex set of build- and run-time dependencies.&lt;/p&gt;
&lt;p&gt;With the existing &lt;code&gt;conda-build&lt;/code&gt; tool, should we want to build this for Python 2.7, 3.4 and 3.5, and against numpy 1.9 and 1.10 (the latest versions of these libraries at the time of writing), things can get a little tedious:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;CONDA_PY=27 CONDA_NPY=19 conda build my_library
CONDA_PY=34 CONDA_NPY=19 conda build my_library
CONDA_PY=35 CONDA_NPY=19 conda build my_library
CONDA_PY=27 CONDA_NPY=110 conda build my_library
CONDA_PY=34 CONDA_NPY=110 conda build my_library
CONDA_PY=35 CONDA_NPY=110 conda build my_library
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With &lt;code&gt;conda-build-all&lt;/code&gt; the special environment variables are taken care of for you (and importantly there is future scope to generalise beyond Python &amp;amp; numpy) and a build matrix is computed:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ conda-build-all my_library
Resolving distributions from &lt;span class="m"&gt;1&lt;/span&gt; recipes... 
Computed that there are &lt;span class="m"&gt;7&lt;/span&gt; distributions from the &lt;span class="m"&gt;1&lt;/span&gt; recipes:
Resolved dependencies, will be built in the following order: 
    my_library-1.0-np19py26_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_library-1.0-np110py27_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_library-1.0-np19py27_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_library-1.0-np110py34_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_library-1.0-np19py34_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_library-1.0-np110py35_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_library-1.0-np19py35_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice how this command is not conceptually equivalent to the original &lt;code&gt;conda-build&lt;/code&gt; calls as I have not asked for particular versions to build against.
&lt;code&gt;conda-build-all&lt;/code&gt; has chosen the top two major versions and within those, the top two minor versions of the packages which require "pinning". Unfortunately, that included Python 2.6, which I didn't really want - to resolve that, we can add extra conditions to our build:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ conda-build-all my_library --matrix-conditions &lt;span class="s2"&gt;&amp;quot;python &amp;gt;=2.7&amp;quot;&lt;/span&gt;
Fetching package metadata: ........
Resolving distributions from &lt;span class="m"&gt;1&lt;/span&gt; recipes... 
Computed that there are &lt;span class="m"&gt;6&lt;/span&gt; distributions from the &lt;span class="m"&gt;1&lt;/span&gt; recipes:
Resolved dependencies, will be built in the following order: 
    my_library-1.0-np110py27_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_library-1.0-np19py27_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_library-1.0-np110py34_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_library-1.0-np19py34_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_library-1.0-np110py35_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_library-1.0-np19py35_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We now have functionally equivalent behaviour that will move forwards as new Python and numpy versions become available.&lt;/p&gt;
&lt;h3&gt;Building multiple recipes in a single call&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;conda-build-all&lt;/code&gt; knows what a conda recipe looks like, and will traverse the directories you give it to look for things to build.&lt;/p&gt;
&lt;p&gt;Supposing we have a directory of recipes which we wish to build, such as the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ find * -name meta.yaml -exec sh -c &lt;span class="s2"&gt;&amp;quot;echo RECIPE: {}; cat {}; echo&amp;quot;&lt;/span&gt; &lt;span class="se"&gt;\;&lt;/span&gt;
RECIPE: my_recipes_directory/my_library/meta.yaml
package:
    name: my_library
    version: &lt;span class="m"&gt;1&lt;/span&gt;.0
requirements:
    build:
        - python
        - numpy x.x
    run:
        - python
        - numpy x.x

RECIPE: my_recipes_directory/my_other_library/meta.yaml
package:
    name: my_other_library
    version: &lt;span class="m"&gt;1&lt;/span&gt;.0
requirements:
    build:
        - python
        - numpy x.x
    run:
        - python
        - numpy x.x
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can simply call &lt;code&gt;conda-build-all&lt;/code&gt; on the directory of recipes to have them built appropriately:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ conda-build-all my_recipes_directory --matrix-conditions &lt;span class="s2"&gt;&amp;quot;python 2.7.*|3.5.*&amp;quot;&lt;/span&gt;
Fetching package metadata: ........
Resolving distributions from &lt;span class="m"&gt;2&lt;/span&gt; recipes... 
Computed that there are &lt;span class="m"&gt;8&lt;/span&gt; distributions from the &lt;span class="m"&gt;2&lt;/span&gt; recipes:
Resolved dependencies, will be built in the following order: 
    my_library-1.0-np110py27_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_library-1.0-np19py27_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_library-1.0-np110py35_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_library-1.0-np19py35_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_other_library-1.0-np110py27_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_other_library-1.0-np19py27_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_other_library-1.0-np110py35_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
    my_other_library-1.0-np19py35_0 &lt;span class="o"&gt;(&lt;/span&gt;will be built: True&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This functionality becomes invaluable when we wish to build many packages, such is the case for the conda-recipes repositories mentioned earlier.&lt;/p&gt;
&lt;h3&gt;Only building the missing distributions&lt;/h3&gt;
&lt;p&gt;The build matrix is supremely useful, but it does come at the cost of the extra time needed to build the many distributions.
With repositories full of recipes, it is easy to come to hundreds of build matrix items. If we want to be able to run &lt;code&gt;conda-build-all&lt;/code&gt; on a regular basis, we can't reasonably expect to build each of those items each time.
Therefore, &lt;code&gt;conda-build-all&lt;/code&gt; has the ability to inspect various locations to determine if a distribution has already been built.
In fact, the default behaviour is to inspect the local conda-build directory to determine if a distribution has already been built locally.
Other options include the ability to inspect conda channels as well as arbitrary local directories.
Supposing we wanted the &lt;code&gt;pelson/channel/testing&lt;/code&gt; channel to have all of the built distributions from &lt;code&gt;my_recipes_directory&lt;/code&gt;, we can use &lt;code&gt;conda-build-all&lt;/code&gt; to good effect:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;conda-build-all my_recipes_directory/ --matrix-conditions &amp;quot;python 2.7.*|3.5.*&amp;quot; \
    --inspect-channels &amp;quot;pelson/channel/testing&amp;quot; \
    --upload-channels &amp;quot;pelson/channel/testing&amp;quot; \
    --no-inspect-conda-bld-directory
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;conda-build-all&lt;/code&gt; is a tool which builds on top of &lt;code&gt;conda-build&lt;/code&gt; to give powerful build-matrix options when building conda distributions.
It has come from &lt;code&gt;ObviousCI&lt;/code&gt;, whose primary objective was to simplify the build and upload of many recipes in a Continuous Integration environment.
In migrating the codebase from &lt;code&gt;ObviousCI&lt;/code&gt; several new test strategies have been developed - making &lt;code&gt;conda-build-all&lt;/code&gt; easier to maintain, and giving rise to the possibility of improving the &lt;code&gt;conda&lt;/code&gt; and &lt;code&gt;conda-build&lt;/code&gt; test suites themselves.&lt;/p&gt;</content><category term="conda"></category></entry><entry><title>Vim search and replace across many files</title><link href="https://pelson.github.io/2015/hints/vim_search_replace/" rel="alternate"></link><published>2015-12-03T12:00:00+00:00</published><updated>2015-12-03T12:00:00+00:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2015-12-03:/2015/hints/vim_search_replace/</id><summary type="html">&lt;p&gt;A powerful combination of commands for search and replace across multiple files with Vim.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;http://vim.wikia.com/wiki/Opening_multiple_files_from_a_single_command-line
http://vim.wikia.com/wiki/Search_and_replace_in_multiple_buffers&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;:arg **/*.cpp

:argdo %s/pattern/replace/ge | update   
&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;A powerful combination of commands for search and replace across multiple files with Vim.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;http://vim.wikia.com/wiki/Opening_multiple_files_from_a_single_command-line
http://vim.wikia.com/wiki/Search_and_replace_in_multiple_buffers&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;:arg **/*.cpp

:argdo %s/pattern/replace/ge | update   
&lt;/pre&gt;&lt;/div&gt;</content></entry><entry><title>Running scripts in temporary conda environments with conda execute</title><link href="https://pelson.github.io/2015/conda_execute/" rel="alternate"></link><published>2015-10-03T12:00:00+01:00</published><updated>2015-10-03T12:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2015-10-03:/2015/conda_execute/</id><summary type="html">&lt;p&gt;Conda is awesome - it is a simple package manager which allows me to create isolated software environments
much like virtualenv. Unlike virtualenv though it can handle any package type, not just python ones.&lt;/p&gt;
&lt;p&gt;The more I use it, the more I want to make use of conda's dependency tracking for …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Conda is awesome - it is a simple package manager which allows me to create isolated software environments
much like virtualenv. Unlike virtualenv though it can handle any package type, not just python ones.&lt;/p&gt;
&lt;p&gt;The more I use it, the more I want to make use of conda's dependency tracking for my own simple scripts to
ensure they were being executed in a suitable environment with the expected dependencies already installed.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;conda build&lt;/code&gt; is an excellent tool for building your own distributions and sharing them on anaconda.org,
but creating a distribution is tiresome if all you have is a single script, rather than a fully-fledged
software package. That is where &lt;code&gt;conda execute&lt;/code&gt; comes in.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;&lt;code&gt;conda execute&lt;/code&gt; allows you to run a script of any kind in a temporary environment defined by metadata in the script itself.&lt;/p&gt;
&lt;p&gt;For example, take the following Python script:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;poisson&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;By adding appropriate &lt;code&gt;conda execute&lt;/code&gt; metadata to our script, we can describe the kind of environment
we would need to be able to run this code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt; &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="n"&gt;my_script&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;

&lt;span class="c1"&gt;#!/usr/bin/env python&lt;/span&gt;

&lt;span class="c1"&gt;# conda execute&lt;/span&gt;
&lt;span class="c1"&gt;# env:&lt;/span&gt;
&lt;span class="c1"&gt;#  - python &amp;gt;=3&lt;/span&gt;
&lt;span class="c1"&gt;#  - numpy&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;poisson&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;conda execute&lt;/code&gt; can now be used to run this script:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ conda execute -v my_script.py

Using specification: 
env: &lt;span class="o"&gt;[&lt;/span&gt;python &amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;, numpy&lt;span class="o"&gt;]&lt;/span&gt;
run_with: &lt;span class="o"&gt;[&lt;/span&gt;/usr/bin/env, python&lt;span class="o"&gt;]&lt;/span&gt;

Prefix: /Users/pelson/miniconda/tmp_envs/ea977067a8fbeb21a594

&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As you can see, the special comment in the script has been read, and an appropriate temporary environment has been created.&lt;/p&gt;
&lt;h2&gt;Temporary environments&lt;/h2&gt;
&lt;p&gt;In order to provision suitable environments for the executed scripts, &lt;code&gt;conda-execute&lt;/code&gt; implements a temporary environment concept.
Rather than adding a new environment in your conda environments directory (and thus filling up the available environments listed in &lt;code&gt;conda env list&lt;/code&gt;), a new "tmp_envs" environments directory has been created, within which &lt;code&gt;conda-execute&lt;/code&gt;'s temporary environments are created (this location is configurable with the &lt;code&gt;conda-execute/env-dir&lt;/code&gt; conda config item).
As you may have noticed in the previous example, where the environment created was named &lt;code&gt;ea977067a8fbeb21a594&lt;/code&gt;, these temporary environments are named by a hashing algorithm (SHA 256, trunkated to 20 characters).
The hash is taken from the &lt;code&gt;conda-execute&lt;/code&gt; metadata of your script, which means that you can re-run a script many times and only need one environment to be created. Additionally, it has the advantage that multiple scripts can share the same environment if their &lt;code&gt;conda-execute&lt;/code&gt; metadata is the same.&lt;/p&gt;
&lt;p&gt;Each time a temporary environment is run with &lt;code&gt;conda-execute&lt;/code&gt; a log entry is added, allowing it to keep track of which environments are still in use. Once an environment has been unused for 25 hours any subsequent &lt;code&gt;conda-execute&lt;/code&gt; call will trigger it to be garbage collected, thus preventing your disk filling up with unneeded temporary environments.&lt;/p&gt;
&lt;h2&gt;Configurability&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;conda-execute&lt;/code&gt; builds on top of &lt;code&gt;conda&lt;/code&gt;'s configuration to allow some customisation in behaviour.
The following &lt;code&gt;condarc&lt;/code&gt; shows the configuration options that are available:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;conda-execute:
    # The directory to use to hold the temporary environments.
    env-dir: &amp;quot;{config.envs_dirs[0]}/../tmp_envs&amp;quot;

    # The number of hours that an environment should be unused for to be
    # considered for garbage collection.
    remove-if-unused-for: 25
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Reproducibility of scripts with &lt;code&gt;conda-execute&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;There are a few really interesting usecases which I'm keen to explore with &lt;code&gt;conda-execute&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I'm already making use of &lt;code&gt;conda-execute&lt;/code&gt; as a form of Makefile for this blog. My &lt;a href="https://github.com/pelson/pelson.github.io/blob/source/make.py"&gt;make.py&lt;/a&gt; is simply a command line wrapper to the appropriate &lt;code&gt;pelican&lt;/code&gt; subcommand:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$&amp;gt; ./make.py --help
usage: Help [-h] {html,publish,reload} ...

positional arguments:
  {html,publish,reload}
    html                Make the html
    publish             Make publishable html, and put it in the
                        output_branch.
    reload              Make the html, and watch the folder for any changes.

optional arguments:
  -h, --help            show this help message and exit
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The concept of creating reproducible scripts goes far wider than trivial Makefiles though - with &lt;code&gt;conda-execute&lt;/code&gt;, because the metadata in the script &lt;strong&gt;is&lt;/strong&gt; the definition of the execution environment, important information about its dependencies and how it is run are all embedded into the script itself.&lt;/p&gt;
&lt;p&gt;I'm particularly keen to explore the reproducibility angle that &lt;code&gt;conda-execute&lt;/code&gt; brings, particularly for scientific applications.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;conda-execute&lt;/code&gt; can be found at &lt;a href="https://github.com/pelson/conda-execute"&gt;github.com/pelson/conda-execute&lt;/a&gt;, and installed with &lt;code&gt;conda install conda-execute --channel conda-forge&lt;/code&gt;.&lt;/p&gt;</content><category term="Python"></category><category term="conda"></category></entry><entry><title>Interactive matplotlib figures in the IPython notebook - they've landed!</title><link href="https://pelson.github.io/2014/nbagg_backend/" rel="alternate"></link><published>2014-06-03T12:00:00+01:00</published><updated>2014-06-03T12:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2014-06-03:/2014/nbagg_backend/</id><summary type="html">&lt;p&gt;{% notebook nbagg_backend/nbagg_backend.ipynb cells[1:2] %}&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;{% notebook nbagg_backend/nbagg_backend.ipynb cells[2:] %}&lt;/p&gt;</summary><content type="html">&lt;p&gt;{% notebook nbagg_backend/nbagg_backend.ipynb cells[1:2] %}&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;{% notebook nbagg_backend/nbagg_backend.ipynb cells[2:] %}&lt;/p&gt;</content><category term="matplotlib"></category><category term="Python"></category></entry><entry><title>Dealing with arrays which are bigger than memory - an intoduction to biggus</title><link href="https://pelson.github.io/2013/massive_virtual_arrays_with_biggus/" rel="alternate"></link><published>2013-09-25T12:00:00+01:00</published><updated>2013-09-25T12:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2013-09-25:/2013/massive_virtual_arrays_with_biggus/</id><summary type="html">&lt;p&gt;{% notebook massive_virtual_arrays_with_biggus/massive_arrays_with_biggus.ipynb cells[:1] %}&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;{% notebook massive_virtual_arrays_with_biggus/massive_arrays_with_biggus.ipynb cells[1:] %}&lt;/p&gt;</summary><content type="html">&lt;p&gt;{% notebook massive_virtual_arrays_with_biggus/massive_arrays_with_biggus.ipynb cells[:1] %}&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;{% notebook massive_virtual_arrays_with_biggus/massive_arrays_with_biggus.ipynb cells[1:] %}&lt;/p&gt;</content><category term="matplotlib"></category><category term="Python"></category><category term="biggus"></category><category term="voluminous data"></category></entry><entry><title>Working with colours in matplotlib</title><link href="https://pelson.github.io/2013/working_with_colors_in_matplotlib/" rel="alternate"></link><published>2013-06-03T12:00:00+01:00</published><updated>2013-06-03T12:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2013-06-03:/2013/working_with_colors_in_matplotlib/</id><summary type="html">&lt;p&gt;When dealing with colours in scientific visualisations some people like to have a colourmap
which can be indexed into to pick specific colours. Whilst this isn't necessarily the best
way of handling colours in matplotlib, it certainly adds a degree of familiarity to users
who have come over from other …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When dealing with colours in scientific visualisations some people like to have a colourmap
which can be indexed into to pick specific colours. Whilst this isn't necessarily the best
way of handling colours in matplotlib, it certainly adds a degree of familiarity to users
who have come over from other visualisation tools, such as IDL.&lt;/p&gt;
&lt;p&gt;In this article I'll cover one approach to using the colour-by-index paradigm in matplotlib.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;{% notebook working_with_colors_in_mpl/working_with_colors.ipynb cells[1:] %}&lt;/p&gt;
&lt;p&gt;This article certainly shows a way of handling the colour-by-index paradigm,
though it must be said that handling colours like this in matplotlib is not
necessarily the best approach - I'll leave that to a future article.&lt;/p&gt;
&lt;p&gt;Find this useful? How do you handle colours in your matplotlib figures? Is there a
killer feature you think matplotlib is missing out on? Let me know via the comments
section.&lt;/p&gt;</content><category term="matplotlib"></category><category term="Python"></category></entry><entry><title>Drawing a pseudo-colour blockplot (pcolormesh) in matplotlib with levels and specific colours</title><link href="https://pelson.github.io/2013/from_levels_and_colors/" rel="alternate"></link><published>2013-05-03T12:00:00+01:00</published><updated>2013-05-03T12:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2013-05-03:/2013/from_levels_and_colors/</id><summary type="html">&lt;p&gt;I recently added a new function to matplotlib to make it easier to draw pseudo-colour
plots given specific levels and colours, in exactly the same way as you can with contour
and contourf.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;{% notebook from_levels_and_colors/using.ipynb cells[1:] %}&lt;/p&gt;</summary><content type="html">&lt;p&gt;I recently added a new function to matplotlib to make it easier to draw pseudo-colour
plots given specific levels and colours, in exactly the same way as you can with contour
and contourf.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;p&gt;{% notebook from_levels_and_colors/using.ipynb cells[1:] %}&lt;/p&gt;</content><category term="matplotlib"></category><category term="Python"></category></entry></feed>